{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Convolutional Neural Networks\n",
    "When your input is shaped \"like a grid\" (think of audio as a 1-D grid, images as a 2-D grid, 3-D scans, ...), there's a good chance it has _local structure_: points closer in the grid are more related to each other than they are to points further away.\n",
    "In images, for example, small patterns of nearby pixels can form simple patterns like edges or corners, or more complex patterns like faces.\n",
    "Pixels in the upper-left of an image are almost unrelated to pixels in the bottom-right corner, except that they're part of the same image.\n",
    "\n",
    "Another observation is that these local patterns can show up anywhere in the image.\n",
    "If you're trying to detect faces, it doesn't really matter where they are in the input.\n",
    "This suggests that we should use the same structure to detect a given pattern everywhere in an image.\n",
    "\n",
    "Convolutions let us take advantage of both of these observations.\n",
    "Instead of connecting every part of the input to every part of the output in the same way, as dense layers do, convolutional layers learn small filters which recognize local patterns and \"slide\" them around the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "The heart of convolutional layers is an operation called \"discrete convolution\" which formalizes how we \"slide filters around the grid.\"\n",
    "\n",
    "## What does \"like a grid\" mean?\n",
    "What this really means is that the input data has some kind of _ordering_ along at least one of its axes.\n",
    "128x128-pixel color images will be represented by tensors of shape [128, 128, 3] -- the first two axes refer to the pixel, and the third picks an RGB **channel**.\n",
    "The channel axis (also sometimes called the depth axis) is not ordered, since red is no closer to green than it is to blue.\n",
    "But the first two axes are ordered: pixel (1, 1) is closer to pixel (1, 2) than it is to (1, 3) or (3, 3).\n",
    "\n",
    "Convolutional layers are designed to take advantage of this ordering, by assuming that close points are more related than further ones.\n",
    "Convolutions will only act on the ordered axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "Instead of a set of dense weights, convolutional layers learn a set of **kernels**, which are formatted like small patches of grid.\n",
    "In 1-D, they're 1-dimensional grid segments, in 2-D they're little squares, and in 3-D they're volumes.\n",
    "\n",
    "To compute the **activation** of a kernel in a particular location in the grid, place the kernel centered there, then compute the dot product of the kernel and the input by multiplying elements that line up and summing the results.\n",
    "\n",
    "The full result of **convolving** a kernel over an input is the set of activations produced by \"sliding\" the kernel to all locations on the input, resulting in a grid shaped like the input.\n",
    "This is often called an **activation map**.\n",
    "\n",
    "One really important thing to notice here is that _convolution preserves order_.\n",
    "When you convolve a kernel over an image, the upper-left of the activation map corresponds to the upper-left of the image.\n",
    "This means that convolutional layers can _preserve local structure_ even as you apply multiple convolutional layers in sequence.\n",
    "Compare this with dense layers, which connect every output to every input and so lose all ordering information.\n",
    "\n",
    "Another important property is that convolutions find small local patterns.\n",
    "You can think of the kernel as a \"template\".\n",
    "Its activation on a patch of image indicates how well that image patch matches the template: high positive values mean they have high \"similarity\" (note that the dot product is not actually normalized), high negative values mean they have similar structure but opposite sign, and values near zero mean that the kernel doesn't structurally match the input there.\n",
    "These \"templates\" are applied everywhere on the input, able to match the same pattern (e.g. an edge or a face) again and again.\n",
    "\n",
    "### 1D\n",
    "![1-D convolution](https://files.realpython.com/media/njanakiev-1d-convolution.d7afddde2776.png)\n",
    "(Image source: [Deep Learning with Python](https://realpython.com/asins/1617294438/))\n",
    "\n",
    "### 2D\n",
    "![2-D convolution gif](https://cdn-images-1.medium.com/max/1440/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
    "\n",
    "In this animation, the input image is the blue grid on the left, the output is the teal grid on the right, and the kernel is the square grid of weights\n",
    "$$\\begin{bmatrix}0 & 1 & 2 \\\\ 2 & 2 & 0 \\\\ 0 & 1 & 2 \\end{bmatrix}$$\n",
    "The activation at a particular location is shown by shading where the kernel is on the input and the resulting space in the output.\n",
    "\n",
    "(Image source: [\"A guide to convolution arithmetic for deep learning\"](https://arxiv.org/abs/1603.07285))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "Kernels act only on a single channel, and produce a single channel.\n",
    "When dealing with multi-channel data (like RGB images, or multi-band satellite imagery), the output of a convolutional layer should be based on all of the channels together.\n",
    "So, instead of using a kernel, we use a **filter**: a collection of kernels, one for each channel in the input's depth axis.\n",
    "\n",
    "![filter](https://cdn-images-1.medium.com/max/1080/1*lRpx5pTrVewFTD8YXjhIKA.png)\n",
    "\n",
    "(Image source: [\"Intuitively understanding convolutions for deep learning\"](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1))\n",
    "\n",
    "The activation of a filter at a point on an image is the sum of the activations of each of its kernels, applied to one of the channels.\n",
    "So, each filter takes in a grid of multiple channels and produces a new grid of a single channel (the activation map).\n",
    "\n",
    "To increase the capacity of a convolutional layer, we use multiple filters in parallel, analogous to the \"width\" or \"number of units\" hyperparameter for dense layers.\n",
    "The activation maps of each filter are concatenated.\n",
    "\n",
    "So, the first layer of a convolutional neural network that operates on 128x128-pixel RGB and uses 64 5x5 filters has the following shapes:\n",
    " - Input images have shape [128, 128, 3] (pixel, pixel, channel)\n",
    " - Each kernel has shape [5, 5] (pixel, pixel)\n",
    " - Each filter has shape [5, 5, 3] (pixel, pixel, channel)\n",
    " - Each filter's activation map has shape [128, 128] (pixel, pixel)\n",
    " - The output of the layer has shape [128, 128, 64] (pixel, pixel, channel)\n",
    "(assuming that the layer is padded so the activation maps are still 128x128 pixels)\n",
    "\n",
    "Note: the next layer's filters would have shape [5, 5, 64] (assuming 5x5 convolutions).\n",
    "Examples typically come in batches, so you can think of each shape also having a batch axis.\n",
    "With a batch size of 8, for example, input image batches would have a shape of [8, 128, 128, 3].\n",
    "\n",
    "For a good visual summary, check out this diagram from [\"A guide to convolution arithmetic for deep learning\"](https://arxiv.org/abs/1603.07285): \n",
    "![multi-channel convolution](./images/convolution_channels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "Since the outputs of a convolutional layer preserve ordering along ordered axes, and multiple filters result in the output having channels, _the outputs have the same format as the inputs_.\n",
    "In this case, an input 128x128 image with 3 channels is turned into an output 128x128 image with 64 channels.\n",
    "This means that we can apply another convolutional layer to the output in order to build deep convolutional neural networks!\n",
    "\n",
    "Much like when we stack dense layers, early convolutional layers capture simple patterns (like edges, corners, and textures) and later convolutional layers use patterns in those patterns to make higher-level features (like object parts and objects).\n",
    "\n",
    "![feature hierarchy](./images/feature_hierarchy.png)\n",
    "\n",
    "(Image source: skymind.ai, [\"A Beginner's Guide to Neural Networks and Deep Learning\"](https://skymind.ai/wiki/neural-network))\n",
    "\n",
    "![feature hierarchy 2](http://teleported.in/post_imgs/11-zeiler-fertus.jpg)\n",
    "(Image source: [\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/abs/1311.2901))\n",
    "\n",
    "This is great for the \"feature extraction\" part of a neural network, but how do we use the final activation maps to do classification or regression?\n",
    "Often, the last activation map is \"flattened\" into a vector, then fed into a dense layer.\n",
    "Usually one or two dense layers, followed by an output layer, are sufficient.\n",
    "\n",
    "For example, if the last convolutional layer of a model is of shape [10, 10, 16], it will be flattened to a vector of shape [10 * 10 * 16] = [1600].\n",
    "Then, every unit in the first dense layer will have 1600 inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting convolutional layers\n",
    "Like dense layers, convolutional layers have lots of interpretations.\n",
    "\n",
    "## Learning filters\n",
    "The idea of convolving filters over images didn't originate with machine learning.\n",
    "In signal processing and image processing, hand-designed kernels are common.\n",
    "For example, hand-designed kernels exist for smoothing, sharpening, and edge detection on images.\n",
    "\n",
    "![filters](./images/filters.png)\n",
    "\n",
    "(Image source: https://en.wikipedia.org/wiki/Kernel_(image_processing))\n",
    "\n",
    "Convolutional layers can be interpreted as learning filters of this sort that produce useful features when convolved over images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A variant of dense layers\n",
    "Mathematically, convolutional layers are equivalent to dense layers with two very important constraints:\n",
    " - Many of the values are set to zero. This corresponds to the property that the pattern a kernel matches is local: it's based on a small number of adjacent inputs, and the weights connecting those inputs to other (non-local) outputs are zero.\n",
    " - The same weights are repeated many times in a fixed pattern. This corresponds to the property that the same kernel is applied many times on the same input.\n",
    "\n",
    "For instance, a dense layer acting on a 4x4 image to produce a 2x2 image requires (4\\*4) \\* (2\\*2) = 64 weights, arranged in a 4x16 matrix (taking in a 16-vector and producing a 4-vector through affine transformation):\n",
    "![dense layer](https://cdn-images-1.medium.com/max/1800/1*Nq-Za2-OzW8J5n7Tu7QIWw.png)\n",
    "\n",
    "However, a 3x3 kernel $K$ acting on this image just requires 3\\*3 = 9 weights.\n",
    "\n",
    "$$K = \\begin{bmatrix}k_{1,1} & k_{1,2} & k_{1 ,3} \\\\ k_{2,1} & k_{2,2} & k_{2 ,3} \\\\ k_{3,1} & k_{3,2} & k_{3 ,3} \\end{bmatrix}$$\n",
    "\n",
    "Convolving this kernel with the input can be represented as multiplying by a specific 4x16 matrix:\n",
    "![convolution as a dense layer](https://cdn-images-1.medium.com/max/1800/1*cr0IabpKu4zIyvDgCTQ64A.png)\n",
    "\n",
    "(Source of images: [\"Intuitively Understanding Convolutions for Deep Learning\"](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1))\n",
    "\n",
    "These properties lead to the interpretation of convolutions as dense layers with three very strong priors:\n",
    " - Patterns are purely local. This is also called the property of having \"sparse interactions.\" This vastly reduces the number of parameters a layer has, making convolutional layers much faster to run and train, as well as being vastly more statistically efficient.\n",
    " - Parameters are shared throughout the layer (called \"weight sharing\" or \"weight tying\"). This is the prior that the same pattern can appear anywhere in an input.\n",
    " - The activation map should be **equivariant** with respect to translation. That means that if something is shifted in an image, the resulting activation is shifted by the same amount in the activation map. \n",
    "\n",
    "These priors work extremely well in practice on images and other kinds of grid-shaped data, including many kinds of time-series data (such as speech waveforms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules / functions\n",
    "One interpretation of the value of weight sharing is that it corresponds to using the same neuron or group of neurons many places in the input like a programmer might write a function and use it many times in a program.\n",
    "\n",
    "From this perspective, forcing the network to apply the same group of neurons multiple times forces it to learn representations that are useful in many ways, leading to robust representation.\n",
    "If the prior that these \"functions\" are a good fit for the input is accurate, then learning one function and applying it many times is much easier than learning the function many times, once in each place it could be applied.\n",
    "In practice this seems to be the case.\n",
    "\n",
    "![modular conv net](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/img/Conv-9-Conv2Conv2.png)\n",
    "\n",
    "In the image above, taken from Chris Olah's [\"Conv Nets: A Modular Perspective\"](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/), the network:\n",
    " 1. Applies a learned function, A, to every point on the input\n",
    " 2. Applies a second learned function, B, to the outputs of A\n",
    " 3. Feeds the outputs of B into a dense layer F, which produces the output.\n",
    "In this view, F is like a large \"block of code\", kind of like a `main()` function, that chooses how to apply the functions A and B by deciding which of their activations on the input are important and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size, receptive field, padding, and stride\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common architecture choices\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses for convolutional networks\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution in TensorFlow\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution in Keras\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
