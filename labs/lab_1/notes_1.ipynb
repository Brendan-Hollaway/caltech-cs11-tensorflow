{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a library (usually used with Python) developed by Google Brain for training and running statistical machine learning models.\n",
    "It is extremely flexible, making almost no assumptions about your model except that you can represent it as a graph of math operations.\n",
    "As a result, it sees lots of use in developing new machine learning models, as well as efficiently training and running existing models.\n",
    "\n",
    "Advantages of TensorFlow over other frameworks:\n",
    " - Most flexible by far -- can use basically any differentiable equation as a model\n",
    " - Most popular and heavily-developed framework by far, used by numerous companies\n",
    " - Works well with CPUs, GPUs, and TPUs (Tensor Processsing Units), and across lots of devices, including distributed training\n",
    " - Incredible debugging and visualization utilities (TensorBoard and tfdbg)\n",
    " - Graph compilation can optimize your code for you\n",
    " - Gives you lots of control over hardware (e.g. whether variables live in CPU or GPU memory)\n",
    " - Open-source!\n",
    "\n",
    "Disadvantages:\n",
    " - Graph programming unfamiliar to many people\n",
    " - Hard to search documentation, lots of deprecated and soon-to-be-deprecated APIs (with TensorFlow 2.0, this will hopefully change)\n",
    " - Generally more verbose and difficult to develop models (but we will also learn Keras, a library meant to make building common models in TensorFlow very simple)\n",
    " - Can be slower than other frameworks for certain models (Apache MXNET)\n",
    " - TensorFlow itself doesn't support every kind of statistical model, most notably \"graphical models\" associated with Bayesian statistics (although, see [bayesflow](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why care?\n",
    "\n",
    "Because it lets you do all of the awesome things machine learning can do:\n",
    " - [Translate between any two human languages](https://code.fb.com/ai-research/laser-multilingual-sentence-embeddings/)\n",
    " - [Play games with superhuman skill](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)\n",
    " - [Make a picture look like it was painted by Picasso](http://genekogan.com/works/style-transfer/)\n",
    " - [Generate realistic human faces](https://blog.openai.com/glow/) \n",
    " - [See through walls with WiFi](https://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612)\n",
    " - [Have your phone make calls for you](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)\n",
    " - [Find every tree in the world (shameless self-plug)](https://medium.com/descarteslabs-team/descartes-labs-urban-trees-tree-canopy-mapping-3b6c85c5c9cc)\n",
    " * ...\n",
    " \n",
    " For example, the people in these pictures never existed but were synthesized by a neural network:\n",
    "![Faces generated by neural network](https://research.nvidia.com/sites/default/files/publications/representative_image_512x256.png)\n",
    "(Image credit: [Nvidia Research, Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://research.nvidia.com/publication/2017-10_Progressive-Growing-of))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of TensorFlow API levels\n",
    "![tensorflow api levels](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n",
    "\n",
    "We'll be focusing on three of these:\n",
    " - Python frontend (or \"operations-level TensorFlow\") lets you build a computational graph by hand for maximum power and flexibility\n",
    " - Keras to build neural networks quickly and easily\n",
    " - Datasets API to load and preprocess data efficiently \n",
    " \n",
    "This covers the two most common tasks: using Keras to build a simple neural network using common kinds of layers (like fully-connected, convolutional, and recurrent layers), and using operations-level TensorFlow to develop a novel model.\n",
    " \n",
    "(Image credit: [Google Developers Blog, Introduction to TensorFlow Datasets and Estimators](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and tensor values\n",
    "To do machine learning, you _really_ need to understand tensors and the terminology surrounding them.\n",
    "You can (for machine learning, at least) think of tensors as multidimensional arrays.\n",
    "\n",
    "![tensors](./images/tensors_diagram.jpg)\n",
    "\n",
    "In TensorFlow, `tf.Tensor` objects act as \"placeholders\" (more on this later) for _tensor values_, which are numpy arrays.\n",
    "I'll try to stick to the notation **\"tensor\"** to mean `tf.Tensor` objects and **\"tensor value\"** to mean a multidimensional numpy array.\n",
    "The terminology around tensors and tensor values is the same.\n",
    "\n",
    "A tensor consists of a number of **scalars** organized in some \"rectangular\" way.\n",
    "Scalars have a [**data type**](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType), usually a real floating-point number (`tf.float32`), integer (`tf.int32`), or boolean (`tf.bool`).\n",
    "To access a particular scalar in a tensor, provide an index in each of its **axes** (also called **dimensions**).\n",
    "The total number of axes a tensor has is its **rank**, and each axis has a fixed **size** which determines how many sub-tensors (of rank one less than the parent) you can get by indexing into just that axis.\n",
    "The total number of scalars a tensor can hold is the product of the sizes of its axes (unless it's rank-zero, then it holds a single value).\n",
    "\n",
    "All of the \"size\" properties of a tensor are encapsulated by its **shape**, which fully specifies how many axes it has, what order they come in, and what their sizes are.\n",
    "A tensor's shape is given by a tuple.\n",
    "For example:\n",
    " - a tensor with shape (3) is a vector (rank-1 tensor) with 3 elements\n",
    " - a tensor with shape (3, 4) is a matrix (rank-2 tensor) with 3 rows and 4 columns\n",
    " - a tensor with shape () is a scalar (rank-0 tensor; you need no indices to uniquely identify its only element)\n",
    " - a tensor with shape (2, 2, 2) (rank-3 tensor) is a 2x2x2 \"data cube\"\n",
    " - a tensor with shape (3, 1) is a matrix (rank-2 tensor) with 3 rows and 1 column; subtle difference from having shape (3)\n",
    " \n",
    "When you provide an index for $n$ axes of a rank-$r$ tensor, you get back a tensor with rank $r - n$, and a shape the same as the original but with the indexed axes removed.\n",
    "Numpy and TensorFlow have roughly the same notation for indexing into tensors to obtain sub-tensors.\n",
    "Similarly, some operations called **reductions** operate across one or more axes of a tensor, collapsing those axes into a scalar summarizing their elements.\n",
    "For instance, `tf.reduce_sum` applied to every axis of a tensor outputs a scalar equal to the sum of its scalars.\n",
    "`tf.reduce_mean` applied to axis 0 of a rank-2 tensor returns a vector that, for each column, contains a scalar of the mean of all of the rows in that column. \n",
    "\n",
    "Matrix transposition (which swaps the rows and columns of a matrix) has a natural extension to tensors: it just reorders their axes.\n",
    "You can \"reshape\" a tensor into a _compatible_ shape, which is to say one where the total number of scalars remains the same.\n",
    "\n",
    "I highly recommend understanding numpy indexing, including some of the advanced stuff you can do (like broadcasting, masking, and `np.where`) since a lot of this transfers to TensorFlow.\n",
    "\n",
    "For a good guide to numpy indexing, read https://realpython.com/numpy-array-programming/.\n",
    "For more on tensors, you can read https://www.tensorflow.org/guide/tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor values:\n",
      "3 \n",
      "\n",
      "[1 2 3] \n",
      "\n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "[[1 2 3]] \n",
      "\n",
      "Tensor shapes:\n",
      "(2, 2)\n",
      "() \n",
      "\n",
      "Tensor rank: 2 \n",
      "\n",
      "Tensor indexing:\n",
      "a: [[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] \n",
      "\n",
      "a[0, 0, 0] = 1\n",
      "a[0, 0, 1] = 2\n",
      "a[0, 1, 0] = 3\n",
      "a[1, 0, 0] = 5 \n",
      "\n",
      "a[0, 0, :] = [1 2]\n",
      "a[:, 0, :] =\n",
      " [[1 2]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Examples of creating tensor values in numpy\n",
    "print('Tensor values:')\n",
    "print(np.array(3), '\\n')           # A rank-zero tensor with shape ()\n",
    "print(np.array([1, 2, 3]), '\\n')   # A rank-one tensor with shape (3)\n",
    "print(np.array([[1, 2],\n",
    "                 [3, 4]]), '\\n')   # A rank-two tensor with shape (2, 2)\n",
    "print(np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]]), '\\n')  # A rank-three tensor with shape (2, 2, 2)\n",
    "print(np.array([[1, 2, 3]]), '\\n') # A rank-two tensor with shape (1, 3)\n",
    "\n",
    "# Example of printing tensor shapes\n",
    "print('Tensor shapes:')\n",
    "print(np.array([[1, 2], [3, 4]]).shape)\n",
    "print(np.array(3).shape, '\\n')\n",
    "\n",
    "# Example of printing tensor rank\n",
    "print('Tensor rank:', \n",
    "      np.array([[1, 2], [3, 4]]).ndim,\n",
    "      '\\n')\n",
    "\n",
    "# Examples of indexing\n",
    "print('Tensor indexing:')\n",
    "a = np.array([[[1, 2],\n",
    "                 [3, 4]],\n",
    "                [[5, 6],\n",
    "                 [7, 8]]])\n",
    "\n",
    "print('a:', a, '\\n')\n",
    "\n",
    "print('a[0, 0, 0] =', a[0, 0, 0])  # Selecting a particular scalar\n",
    "print('a[0, 0, 1] =', a[0, 0, 1])\n",
    "print('a[0, 1, 0] =', a[0, 1, 0])\n",
    "print('a[1, 0, 0] =', a[1, 0, 0], '\\n')\n",
    "\n",
    "print('a[0, 0, :] =', a[0, 0, :])   # Take the first value on axes 0 and 1, \n",
    "                                    # leaving axis 2 alone\n",
    "print('a[:, 0, :] =\\n', a[:, 0, :]) # Take the first value on axis 2, \n",
    "                                    # leaving axes 0 and 2 alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable programming\n",
    "Differentiable programming is a paradigm in which a program is represented as a **dataflow graph**: _mathematical computations organized as a directed graph_:\n",
    "![a computational graph](https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png)\n",
    "\n",
    "(Image credit: [Chris Olah's blog, Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/))\n",
    "\n",
    "Nodes in the graph are operations, and edges are tensors, which flow between operations (hence the name).\n",
    "When you want to evaluate an operation (say, to get its output tensor), all of the operations that produce tensors it depends on (found by stepping back one layer in the graph) are evaluated, and so on recursively.\n",
    "In this way, you can define a bunch of interconnected computations as a single graph, and when you want to compute a value, TensorFlow will run _only the computations it needs to to compute your value_.\n",
    "During a single evaluation, tensors have fixed values, so TensorFlow will cache that value in order to prevent computing the same thing multiple times.\n",
    "For instance, when evaluating \"e\" in the above graph, the value of \"b\" is computed only once despite being used twice.\n",
    "\n",
    "It's very useful to keep the graph formalism in mind when writing TensorFlow code, and we'll return to the core ideas of differentiable programming again and again in this course.\n",
    "Because you completely define the graph before running it, TensorFlow can also [optimize the graph with the XLA compiler](https://www.tensorflow.org/xla/), doing things like fusing operations and doing clever memory-saving hacks.\n",
    "\n",
    "The key feature that separates differentiable programming from ordinary graph programming is that in graph programming, _you only use differentiable operations_.\n",
    "What this means is that you can use the chain rule (see the next lecture, on the Backpropagation algorithm) to compute _the gradient of any value in the graph with respect to any other value in the graph_.\n",
    "This is incredibly powerful!\n",
    "\n",
    "Critically, you can use _gradient-based optimization methods_ to maximize or minimize values in the graph by changing variable tensors earlier in the graph.\n",
    "This is how we train machine learning models using TensorFlow.\n",
    "The most common family of such optimization algorithms is... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Gradient descent is a simple and powerful algorithm for minimizing differentiable functions.\n",
    "I'll describe it only briefly here because there are [plenty](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent) [of](https://work.caltech.edu/library/101.html) [good](https://hackernoon.com/gradient-descent-aynk-7cbe95a778da) [explanations](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) [online](https://www.youtube.com/watch?v=jc2IthslyzM).\n",
    "\n",
    "For a differentiable function $f(\\vec{x})$, the gradient $\\nabla_\\vec{x} f$ gives the direction of steepest increase of $f$ -- the direction to move the parameters $\\vec{x}$ that increases $f$ the most, in a small region around the value of $\\vec{x}$. \n",
    "If we want to minimize $f$, we can therefore keep track of a \"current value of $\\vec{x}$\" and repeatedly take small steps in the opposite direction of the gradient.\n",
    "\n",
    "The size of step is the **learning rate**, denoted $\\alpha$.\n",
    "It should be small, or else we'll overshoot the area where we can assume the gradient really gives the direction of steepest increase (where the linear term in the Taylor series of the function no longer dominates).\n",
    "For steep gradients, we take bigger steps because we have a stronger signal of where the parameters $\\vec{x}$ should go.\n",
    "For $\\alpha$ too small, we converge too slowly to a minimum (by taking very many small steps).\n",
    "For $\\alpha$ too large, we may not be able to minimize the function at all.\n",
    "\n",
    "![gradient descent visualization](./images/gradient_descent.jpg)\n",
    "\n",
    "In machine learning, we define a value that quantifies how badly our model fits the data (the \"loss\" or \"training error\"), then use gradient descent to change the parameters of our model (made possible by differentiable programming) to minimize that value.\n",
    "\n",
    "Below is some example code that computes the gradients and applies the updates by hand to minimize $f(x) = x^2$.\n",
    "TensorFlow will do this for you (and one of the problems on the first lab is to minimize a simple function with TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of x: 0.00026561398887587435\n",
      "Final value of f(x): 7.05507910865531e-08\n"
     ]
    }
   ],
   "source": [
    "# An example of minimizing the function f(x) = x^2 with gradient descent\n",
    "def f(x): return x**2\n",
    "def grad_f(x): return 2*x # Gradient of f(x), computed by hand\n",
    "\n",
    "x = 10\n",
    "learning_rate = 0.05\n",
    "x_values = [x]\n",
    "y_values = [f(x)]\n",
    "\n",
    "for step in range(100):\n",
    "    x = x - learning_rate * grad_f(x)\n",
    "    x_values.append(x)\n",
    "    y_values.append(f(x))\n",
    "    \n",
    "plt.figure(figsize=(5, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(101), x_values, 'b-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(101), y_values, 'r-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "print('Final value of x:', x)\n",
    "print('Final value of f(x):', f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on local minima \n",
    "Gradient descent is only guaranteed to converge to the global minimum of a _convex_ function.\n",
    "This seems like a problem, since most loss functions we want to minimize are not convex.\n",
    "However, local minima are very rare in high-dimensional space: for a random function to be a local minimum in $n$-dimensional space, it needs to curve upwards on _every_ axis, so it has probability $\\propto 2^{-n}$.\n",
    "For neural networks, the vector space of possible parameter values can have millions of parameters.\n",
    "\n",
    "Instead, we usually care about _saddle points_, where the loss function looks locally flat.\n",
    "Modern variants of gradient descent like momentum, adagrad, and Adam approach this problem by using more information than just the gradient to make parameter updates.\n",
    "But, how to handle these points best when training neural networks (and even how much of a problem they are) is still somewhat of an open problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph programming in TensorFlow\n",
    "TensorFlow programs are divided into two halves: building the graph and evaluating subgraphs of the graph.\n",
    "\n",
    "In the first step, a `tf.Graph` represents a complete computational graph, consisting of `tf.Operation` nodes and `tf.Tensor` edges.\n",
    "You can create multiple graphs, but I suggest always using the default one (at least for now).\n",
    "When you call a function corresponding to a TensorFlow operation, what you're really doing is _adding that operation to the graph_ -- the function returns a `tf.Tensor`, which you can use as inputs to other operations to chain them together in a graph.\n",
    "So, you should call each function _exactly once per node you want in your graph_, or you'll wind up with duplicates and buggy code.\n",
    "\n",
    "As a general rule, calling a function **adds an operation to the graph** and **returns a tensor** which you can use as input for one or more operations.\n",
    "Recall that a tensor is just a placeholder for a tensor value, and does not have a value until it is evaluated in a run.\n",
    "\n",
    "Operations have names, which are passed down to the tensors they create.\n",
    "_You should always set the names of your important operations!_\n",
    "Operations can also live inside \"name scopes\", which add a common prefix to all enclosed operation names and allows for collapsing them into a single node in TensorBoard, defined in terms of its inputs and outputs.\n",
    "_Use name scopes where appropriate_.\n",
    "Think of names and name scopes like commenting: they're a little thing that will make debugging, maintenance, and development much easier, especially once you start using TensorBoard and tfdbg.\n",
    "\n",
    "To include fixed values in your graph, use the `tf.constant` operation (yes, it's an operation) that returns a constant tensor.\n",
    "To include values that can change between runs, like trainable parameters of a model, you want to create `tf.Variables`, which are a type of tensor.\n",
    "The best way to use variables is to call [tf.get_variable()](https://www.tensorflow.org/api_docs/python/tf/get_variable), which takes a **name** for the variable and an **initializer**.\n",
    "Before using a variable, you need to run its initializer at least once, either through `my_variable.initializer` or `tf.global_variables_initializer()`, which initializes all variables and is usually preferred.\n",
    "\n",
    "For experimentation, consider using a `tf.placeholder`, which takes a datatype and a shape and can be \"fed\" with values during execution.\n",
    "\n",
    "You should really read [the official TensorFlow guide on graphs and sessions](https://www.tensorflow.org/guide/graphs), or at least skim it.\n",
    "The [guide on variables is worthwhile too](https://www.tensorflow.org/guide/variables).\n",
    "Look up and read the documentation on constants, variables, and placeholders.\n",
    "\n",
    "In the second step, a `tf.Session` represents the _context of a computation_, and is required to evaluate tensors.\n",
    "The `run` function of a session, given a tensor or list of tensors, recursively evaluates operations to eventually return the tensor's value.\n",
    "It can also take a dictionary of \"feeds\", which can replace the value of any tensor in the graph with an alternative value.\n",
    "This is usually used to feed placeholders, but you can overwrite any tensor value this way.\n",
    "\n",
    "Optimization is performed by instantiating an optimizer with necessary parameters, such as `optimizer = tf.train.GradientDescentOptimizer(0.01)` for simple gradient descent with a learning rate of 0.01.\n",
    "Then, `optimize_step = optimizer.minimize(loss)` calls an operation function that takes in a tensor `loss` and returns a value `optimize_step` that, when evaluated, returns nothing, but changes the variables in your graph to minimize the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "TensorBoard is a visualization and debugging utility that runs as a command-line program hosting a local website at http://localhost:6006.\n",
    "\n",
    "To use it, you need a `tf.summary.FileWriter` to write log files locally.\n",
    "\n",
    "If you give it a graph on initialization, it will visualize everything added to the graph -- otherwise, call its `add_graph()` function later.\n",
    "This allows for visualizing your whole computational graph in the \"Graphs\" tab.\n",
    "Expand and collapse composite nodes by double-clicking.\n",
    "\n",
    "To plot values from the graph, you can use \"summary operations\" which return log files when run.\n",
    "Typically, you just add the summary operations to the graph, then save an operation obtained by `tf.summary.merge_all()`, which returns a single composite summary object when run.\n",
    "If you run that operation through a seession, it returns a summary file that you then pass to `writer.add_summary(...)` to log all of the scalars at once.\n",
    "\n",
    "If you're interested, read [the official TensorBoard Guide](https://www.tensorflow.org/guide/summaries_and_tensorboard) which goes into much more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full program example\n",
    "Below is an example of performing the minimization of $f(x) = x^2$ from before, including logging the graph for visualization, as well as plotting $x$ and $f(x)$ throughout training.\n",
    "If you run it locally, then run `tensorboard --logdir lecture_logs` and navigate to http://localhost:6006, you can view these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name f(x) is illegal; using f_x_ instead.\n"
     ]
    }
   ],
   "source": [
    "# Create FileWriter\n",
    "writer = tf.summary.FileWriter('./lecture_logs')\n",
    "\n",
    "# Add operations to the default graph\n",
    "with tf.name_scope('minimize_x_squared'):\n",
    "    # Create input variable\n",
    "    x = tf.get_variable('x', shape=(), dtype=tf.float32, \n",
    "                        initializer=tf.constant_initializer(10)) \n",
    "    \n",
    "    # Compute f(x) = x^2\n",
    "    # `tf.pow` takes in two tensors and returns a tensor\n",
    "    y = tf.pow(x, tf.constant(2.0), name='y')\n",
    "    \n",
    "    # Create optimization operator\n",
    "    optimize_step = tf.train.GradientDescentOptimizer(0.05, name='optimizer').minimize(y)\n",
    "    \n",
    "    # Create summary operations\n",
    "    tf.summary.scalar('x', x) # Adds operations without giving us a handle to them\n",
    "    tf.summary.scalar('f(x)', y)\n",
    "    merged_summary = tf.summary.merge_all() # This operation runs all summary ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of x: 10.0\n",
      "Initial value of f(x): 100.0\n",
      "Final value of x: 0.00026561393\n",
      "Final value of f(x): 7.0550755e-08\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "with tf.Session() as sess: # Create a session\n",
    "    writer.add_graph(sess.graph) # Write the graph to log files\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer()) # Initialize all variables\n",
    "    \n",
    "    print('Initial value of x:', sess.run(x))\n",
    "    print('Initial value of f(x):', sess.run(y))\n",
    "    \n",
    "    for step in range(100):\n",
    "        sess.run(optimize_step)\n",
    "        summary = sess.run(merged_summary)\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "    print('Final value of x:', sess.run(x))\n",
    "    print('Final value of f(x):', sess.run(y))\n",
    "    \n",
    "    writer.flush() # Ensure that the writer has written logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper graph example\n",
    "This code creates the computational graph pictured above as a TensorFlow graph, showcasing overloading on arithmetic operators, using placeholders, and bigger graph visualization in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('example_graph'):\n",
    "    a = tf.placeholder(tf.float32, (), name='a') # Returns a tensor that must be fed\n",
    "    b = tf.placeholder(tf.float32, (), name='b')\n",
    "    c = a + b # This is shorthand for the operator function tf.add(a, b)\n",
    "    d = b + tf.constant(1.0)\n",
    "    e = c * d\n",
    "    \n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1.0\n",
      "a = 2.0\n",
      "e = 9.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('a =', sess.run(a, feed_dict={a: 1}))\n",
    "    print('a =', sess.run(a, feed_dict={a: 2}))\n",
    "    print('e =', sess.run(e, feed_dict={a: 1, b: 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whew! That's all the basics of writing code in TensorFlow. More interesting problems and models to come :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
