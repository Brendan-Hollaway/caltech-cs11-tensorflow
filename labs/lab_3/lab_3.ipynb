{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mapping with Fully-Connected Networks\n",
    "Blocks of dense layers are really good at memorizing patterns.\n",
    "This week we'll step outside the traditional \"prediction\" regime of machine learning and use a fully-connected network (FCN) to represent a 2-D scene in its weight by interpolating between known points.\n",
    "\n",
    "Throughout, you should be impressed by two (related) properties of dense neural nets:\n",
    " - They have high enough capacity to entirely memorize complex patterns\n",
    " - They are sufficiently nonlinear to build these patterns up from simple inputs (like (x, y) coordinates in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointcloud mapping\n",
    "\n",
    "![full scene](./images/full_scene.png)\n",
    "\n",
    "### The problem\n",
    "Imagine there is a square (2-D) room with four walls and several square and circular objects in it.\n",
    "We put laser distance sensors around the room, which shoot rays that continue until they hit an object or a wall and report the distance, with limited angular and radial resolution.\n",
    "We want to use this limited data to produce a full 2-D reconstruction of the scene, filling in points the sensors haven't observed with likely values (in essence, interpolating).\n",
    "\n",
    "### The data\n",
    "We know that every ray that there's an object at the point each ray hit, and we also know that there's no object between the camera and the contact point.\n",
    "The data is provided as (x, y) pairs, where some points correspond to places we know are on object boundaries (shown in green above) and points we know are not inside objects (shown in red above).\n",
    "\n",
    "These points come in two data files: `data/positives.txt` for points on the edge of objects, and `data/negatives.txt` for points outside of objects.\n",
    "To generate these files, run `make_data.py`.\n",
    "\n",
    "(Check out the code if you want to see how the points are generated, but don't change anything!)\n",
    "\n",
    "### The model\n",
    "We want to store a full description of the scene in the weights of a neural network.\n",
    "To do this, we'll train a FCN to classify (x, y) points by whether or not they are the edge of a wall or object.\n",
    "Then, we can draw the scene by sampling (x, y) pairs and drawing the model's output.\n",
    "So, the network has \"memorized\" where the objects and walls are, and what their shapes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load the data\n",
    "Load the data using `np.loadtxt()` (default arguments should work), then convert it to a usable format with a feature array and label array, which is 1 if a point is the edge of an object and 0 if it is not.\n",
    "\n",
    "Functions to look at:\n",
    " - `np.loadtxt`\n",
    " - `np.concatenate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Build a data pipeline\n",
    "Set up a `tf.data.Dataset` and a `tf.data.Iterator`.\n",
    "This time, there's no test set, just a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build a model graph\n",
    "The model is a simple fully-connected neural network with four hidden layers, each with 64 units, and an output layer that performs binary classification with sigmoid activation.\n",
    "Recall that the input is just a pair (x, y).\n",
    "\n",
    "Functions to look at throughout:\n",
    " - `tf.cast`\n",
    " - `tf.expand_dims`\n",
    " - `tf.squeeze`\n",
    " - arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Input tensors\n",
    "Get a tensor for the input coordinates and for the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Helper function to make dense layers\n",
    "It's tedious (and bad design) to explicitly write out every variable, matrix multiplication, etc.\n",
    "Instead, it's common to define helper functions that create one of a structure you expect to repeat in your model.\n",
    "\n",
    "Write a function called `make_dense_layer(...)`, which has its signature and scopes defined as a stub below.\n",
    "When called, it should add variables and operations to the graph which implement a single dense layer.\n",
    "\n",
    "If `do_activation` is True, it should apply ReLU activation to the computed values.\n",
    "To do this, you can use `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def make_dense_layer(prev_activations, dim_input, dim_output, \n",
    "                     do_activation=True, postfix=''):\n",
    "    '''\n",
    "    Adds a dense layer to the model graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prev_activations: tensor\n",
    "        The activations of the previous layer, or \n",
    "        the input for the first dense layer.\n",
    "    dim_input: int\n",
    "        Number of features in the input representation.\n",
    "    dim_output: int\n",
    "        Number of features in the output representation.\n",
    "        Equivalently, number of units in this layer.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    postfix: string\n",
    "        Postfix on name and variable scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    with tf.name_scope('dense' + postfix):\n",
    "        with tf.variable_scope('dense' + postfix):\n",
    "            # Define variables here\n",
    "        # Define operations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Make dense layers\n",
    "Use the helper function to make four dense layers.\n",
    "Each should have 64 units, the correct input dimensions, and a distinct (and meaningful) postfix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Compute logit\n",
    "Use the helper function to make a final dense layer with `dim_output=1` to compute the final logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Compute class probability for output\n",
    "Use `tf.sigmoid` to compute the probability that the input coordinate is a boundary point.\n",
    "We will not use this for the loss, just for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6: Compute weighted cross-entropy loss\n",
    "There are about 33 times as many non-boundary coordinates as there are boundary coordinates.\n",
    "To prevent the network from just learning to predict that every coordinate is not a boundary, we should weight the loss function to consider boundary pixels more heavily.\n",
    "\n",
    "To do this, use `tf.nn.weighted_cross_entropy_with_logits()` on the logits computed before, plus the input label, and a positive weight of 20.\n",
    "Then, produce the mean loss for the batch with `tf.reduce_mean`, and add a summary scalar to plot loss in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.train.MomentumOptimizer` with `lr=1e-3` and `momentum=0.9`).\n",
    "Then, explicitly compute the gradients in a variable called `gradients` (so we can plot them below) and make an operation that applies those gradients to the graph.\n",
    "\n",
    "Note: don't use `optimizer.minimize`, which will result in computing the gradients twice since we already have them from `compute_gradients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8: Plot variables and gradients\n",
    "You don't need to do this for every model, but it's illustrative to do at least once.\n",
    "I've written the code for this below, with a pattern borrowed from http://matpalm.com/blog/viz_gradient_norms/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('visualization'):\n",
    "    for gradient, variable in gradients:\n",
    "        if variable.name and gradient is not None:\n",
    "            tf.summary.histogram('gradients/' + variable.name, tf.norm(gradient))\n",
    "            tf.summary.histogram('variables/' + variable.name, tf.norm(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: train the model\n",
    "Same as last time, except there's no test set.\n",
    "You don't need to distinguish between epochs and steps here, but do run the summary ops and log the summaries.\n",
    "\n",
    "I used 50 epochs, and got a final cross-entropy loss of about 0.3.\n",
    "You might not need to use this many epochs to get reasonable results, but more will get crisper results.\n",
    "\n",
    "This might take a while on CPUs, so if it's taking too long, think about:\n",
    " - Using Google Colaboratory\n",
    " - Increasing your batch size\n",
    " - Reducing the frequency with which you run summary ops, save metadata, etc\n",
    " \n",
    "Watching plots, histograms, etc in TensorBoard as the model trains can be fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Plot the model\n",
    "Let's see what the network learned.\n",
    "Generate 100 x values and 100 y values, each in the range (-1, 1).\n",
    "Then compute the value of `probability` using every coordinate in that grid (every pairwise combination of x and y values), and plot a \"map\" of those probabilities using `plt.scatter()` (check out the `c` argument).\n",
    "\n",
    "You can use `feed_dict` to pass in data, since efficiency matters less here than in training.\n",
    "I used it to overwrite the input coordinates, since I didn't define a `Dataset` here.\n",
    "But, there are multiple ways to do this (e.g. defining a new `Dataset` of coordinates and fake labels).\n",
    "\n",
    "Remember to load the trained weights before running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: TensorBoard plots\n",
    "This time, we have some more interesting TensorBoard plots than last time.\n",
    "Run TensorBoard, and look at:\n",
    " - The model graph in Graphs\n",
    " - The loss over time in Scalars\n",
    " - The weights, biases, and gradients over time in Distributions and Histograms\n",
    " \n",
    "The loss should decrease over time, and eventually stabilize (with some noise).\n",
    "\n",
    "The gradients for weights and biases may not go to zero, and your weights and biases may not look like they're converging.\n",
    "This is because of \"bias drift\" where multiple possible settings of weights and biases have the same loss value.\n",
    "To fix this, we could add a term to the loss which penalizes the model slightly based on the magnitude of the bias terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Again, but in Keras\n",
    "Make a Keras Sequential model which is equivalent to the one we've trained in TensorFlow.\n",
    "Train it on the same dataset, and again plot its output in the same way. \n",
    "\n",
    "Hints:\n",
    " - This won't take nearly as long as the previous sections\n",
    " - You don't need to use `keras.utils.to_categorical` since this is binary classification.\n",
    " - The appropriate loss function is 'binary_crossentropy'\n",
    " - To implement the same loss weighting, use `class_weight={0: 1.0, 1:20.0}` in `model.fit()`\n",
    " - You can get the outputs of the model over the entire grid at once with `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1: Build a model\n",
    "Use `keras.models.Sequential` to build the same FCN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: Define an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3: Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4: Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
